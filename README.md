# Transformer Encoder Architecture (from Scratch)

This repository contains a NumPy-based implementation of a Transformer Encoder module from scratch — inspired by the original Transformer architecture described in ["Attention Is All You Need"](https://arxiv.org/abs/1706.03762).

---

## 🚀 Project Highlights

- ✅ **Word Tokenization and Embedding**
- ✅ **Positional Encoding (Sinusoidal)**
- ✅ **Scaled Dot-Product Attention**
- ✅ **Multi-Head Self Attention**
- ✅ **Residual Connections & Layer Normalization**
- ✅ **Position-wise Feed-Forward Network**
- ✅ **Stacked Encoder Layers**

This implementation is **minimal**, **educational**, and built **without using PyTorch or TensorFlow** — just **NumPy**.

---

## 🧠 Why This Project?

The aim of this project is to **understand the inner workings** of the Transformer encoder block by manually implementing each component step-by-step — reinforcing your understanding of:

- Self-Attention mechanism  
- Positional encoding  
- Multi-head attention  
- Feed-forward layers  
- Residual connections and layer normalization

---

## 📁 File Structure

```bash
📦 transformer-encoder-architecture
 └── encoder.py          # Full implementation of the encoder module


