# Transformer Encoder Architecture (from Scratch)

This repository contains a NumPy-based implementation of a Transformer Encoder module from scratch ‚Äî inspired by the original Transformer architecture described in ["Attention Is All You Need"](https://arxiv.org/abs/1706.03762).

---

## üöÄ Project Highlights

- ‚úÖ **Word Tokenization and Embedding**
- ‚úÖ **Positional Encoding (Sinusoidal)**
- ‚úÖ **Scaled Dot-Product Attention**
- ‚úÖ **Multi-Head Self Attention**
- ‚úÖ **Residual Connections & Layer Normalization**
- ‚úÖ **Position-wise Feed-Forward Network**
- ‚úÖ **Stacked Encoder Layers**

This implementation is **minimal**, **educational**, and built **without using PyTorch or TensorFlow** ‚Äî just **NumPy**.

---

## üß† Why This Project?

The aim of this project is to **understand the inner workings** of the Transformer encoder block by manually implementing each component step-by-step ‚Äî reinforcing your understanding of:

- Self-Attention mechanism  
- Positional encoding  
- Multi-head attention  
- Feed-forward layers  
- Residual connections and layer normalization

---

## üìÅ Core Components

| Component                        | Description                                                         |
| -------------------------------- | ------------------------------------------------------------------- |
| **Tokenization**                 | Splits the sentence into lowercase word tokens                      |
| **Word Embedding**               | Randomly initializes word vectors                                   |
| **Positional Encoding**          | Adds sinusoidal patterns to capture position                        |
| **Q, K, V Matrices**             | Projects input embeddings to Query, Key, and Value vectors          |
| **Scaled Dot-Product Attention** | Calculates attention weights and combines Value vectors accordingly |
| **Multi-Head Attention**         | Runs multiple parallel attention heads                              |
| **Projection Layer**             | Concatenates all heads and applies a linear transformation          |
| **Residual + LayerNorm**         | Applies skip connections and normalizes                             |
| **Feed-Forward Network**         | Two-layer MLP with ReLU activation in between                       |
| **Encoder Stack**                | Repeats the encoder block for N layers (default = 8)                |

## üôå Acknowledgments
Attention Is All You Need (Vaswani et al.)

BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

Blogs and tutorials from the deep learning community

## Connect With Me
If you're interested in deep learning, NLP, or building projects like this ‚Äî let's connect!

üìß vickyvk5156@gmail.com

