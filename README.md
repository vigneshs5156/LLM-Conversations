# Transformer Encoder Architecture (from Scratch)

This repository contains a NumPy-based implementation of a Transformer Encoder module from scratch â€” inspired by the original Transformer architecture described in ["Attention Is All You Need"](https://arxiv.org/abs/1706.03762).

---

## ğŸš€ Project Highlights

- âœ… **Word Tokenization and Embedding**
- âœ… **Positional Encoding (Sinusoidal)**
- âœ… **Scaled Dot-Product Attention**
- âœ… **Multi-Head Self Attention**
- âœ… **Residual Connections & Layer Normalization**
- âœ… **Position-wise Feed-Forward Network**
- âœ… **Stacked Encoder Layers**

This implementation is **minimal**, **educational**, and built **without using PyTorch or TensorFlow** â€” just **NumPy**.

---

## ğŸ§  Why This Project?

The aim of this project is to **understand the inner workings** of the Transformer encoder block by manually implementing each component step-by-step â€” reinforcing your understanding of:

- Self-Attention mechanism  
- Positional encoding  
- Multi-head attention  
- Feed-forward layers  
- Residual connections and layer normalization

---

## ğŸ“ File Structure

```bash
ğŸ“¦ transformer-encoder-architecture
 â””â”€â”€ encoder.py          # Full implementation of the encoder module


